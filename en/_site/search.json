[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Goal",
    "section": "",
    "text": "From Atelier de código, we want to support you on your journey into data analysis. For those ready to dive in, we offer R courses for beginners (and not-so-beginners as well), covering programming fundamentals, data cleaning and organization, statistical analysis, and visualization. For those who already have a foundation but need someone to think things through with, we provide tutoring sessions where we advise you on your analysis. And for those who don't have the time (or the inclination!) to learn programming, we analyze your data and deliver a tailored report."
  },
  {
    "objectID": "posts/grammar-graphics-ggplot.html",
    "href": "posts/grammar-graphics-ggplot.html",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "The grammar of graphics: thinking of visualization as a language\nGraphics hold a central place in data analysis. They are used to explore patterns, communicate results, and support arguments. However, they are often thought of as a final product, something that is “chosen” from a set of available options. The idea of a grammar of graphics proposes a shift: graphics can be understood as constructions composed of parts or layers, organized according to relatively stable rules, comparable to how languages work.\nInstead of asking “what type of chart should I make”, the question becomes “what do I want to convey”. This will allow us to think about what relationships we want to represent and what elements we need to combine to make them visible. Visualization ceases to be decorative and becomes integrated into analytical reasoning.\n\nWhat is understood by the grammar of graphics\nThe concept of the grammar of graphics was developed by Leland Wilkinson in his book The Grammar of Graphics and starts from a simple idea: any statistical graphic can be decomposed into basic components. These include data, variables mapped to visual properties, geometric shapes that represent that data, and scales that translate values into positions, colors, or sizes.\nFrom this approach, a graphic is not an indivisible object, but the result of a series of explicit decisions. Which variable goes on the horizontal axis, which on the vertical, whether values are represented by points, bars, or lines, how observations are grouped. Each of these choices is part of the graphic’s structure and affects its interpretation.\n\n\nThe grammar of graphics in ggplot2\nHadley Wickham revisits this concept in his article A Layered Grammar of Graphics and introduces his ggplot2 package which implements this idea directly. To build a graphic, one starts with a dataset and adds layers, each with a specific function. The code reflects this compositional logic and allows the graphic to be read as a sequence of decisions.\nA minimal example can illustrate this structure:\nggplot(data = datos, aes(x = edad, y = ingreso)) +\n  geom_point()\nIn this snippet, several key components appear. ggplot() defines the dataset and the basic mapping, i.e., which variables are associated with which visual dimensions. geom_point() adds a concrete geometry, in this case points. If we wanted to change the form of representation, we could replace or add geometries without redoing the entire graphic.\nThis way of working favors controlled experimentation. It is possible to modify just one part of the graphic and observe how the result changes. It also facilitates comparing visualizations that share a common structure, which is especially useful in exploratory analyses.\n\n\nVisualizing as part of the analysis\nThinking of visualization as a language implies recognizing that graphics are not neutral. Each graphic emphasizes certain relationships and relegates others to the background. The grammar of graphics forces us to make these choices explicit, both in the code and in the reasoning that accompanies it.\nIn social analysis contexts, this explicitness is relevant. Visualizing distributions, comparing groups, or tracking temporal evolutions involves defining categories, scales, and units of analysis. The grammar of graphics provides a framework for making these decisions visible and for discussing them.\nFurthermore, by working with code, the graphic becomes reproducible and revisable. Others can read how it was constructed, modify it, or adapt it to new questions. Visualization ceases to be a conclusion of the analysis and becomes part of the process.\n\n\nA tool for learning to read graphics\nThe grammar of graphics not only serves to produce visualizations, but also to read them critically. Identifying which variables are being compared, what scales are used, and what geometries are involved allows for better evaluation of someone else’s graphic and understanding what it shows and what it hides.\nFrom this perspective, learning ggplot2 is not just learning a syntax. It is incorporating a way of thinking about visualization as a situated analytical practice, with rules, possibilities, and limits. In the next posts in the series, we will work with concrete examples to see how this grammar comes into play in common data analysis graphics."
  },
  {
    "objectID": "posts/integrated-development-environment-ide.html",
    "href": "posts/integrated-development-environment-ide.html",
    "title": "Integrated Development Environments (or IDEs, for friends)",
    "section": "",
    "text": "When someone starts programming, one of the first common confusions relates to the working environment. Where does the code “live”? Is it written in a common text file? Is it executed from a console (and what is a console)? Is R the same as RStudio? Do I have to install both things? In what order? These questions are not minor, because they refer to a key distinction: the difference between a programming language and the environment in which that language is used. At this point, the notion of an IDE appears, short for Integrated Development Environment, or integrated development environment.\nAn IDE is a program that brings together several necessary programming tools in one space. Generally speaking, it combines at least four components: a code editor, a console where instructions are executed, tools for exploring files and objects, and aids for writing and reading code. The IDE does not replace the programming language, but rather offers a graphical interface that facilitates its use. R remains R, with its syntax and rules, regardless of the IDE used. However, the work experience changes substantially depending on the chosen environment.\nThere are widely used IDEs across different languages. Visual Studio Code is one of the most popular today and is used for working with multiple languages, from R and Python to JavaScript. Eclipse has a long tradition in Java development. Spyder is common in the Python ecosystem, especially in scientific contexts. In the case of R, the most widespread IDE is RStudio, which was specifically designed to work with this language and with common data analysis practices. In recent years, Positron also appeared, an IDE developed by Posit that reuses many of RStudio’s ideas and articulates them with a more generalist logic, oriented to working with multiple languages. However, in this series, we will focus on RStudio because it remains the most widespread environment in the academic use of R and because its design is deeply aligned with common practices of analysis, teaching, and reproducible writing in this language. And also because it’s what we’re used to :)\nRStudio is organized into an interface that, at first glance, can be overwhelming: panels, tabs, buttons, windows. However, this arrangement responds to a work logic worth understanding. In one panel, you typically find the script editor, where code is written. In another, the console, which allows executing lines or blocks of code and immediately seeing their results. Other panels show the objects available in the session, project files, generated plots, or function documentation. This organization is not merely decorative. It proposes a way of thinking about data work as an activity distributed among writing, execution, exploration, and reading. In this post there is a description of what each of these parts looks like. The important thing is to recognize each unit separately, because eventually the user can rearrange the position of the panels on their screen.\nThe script editor occupies a central place. There, the code is presented as text that can be read, reviewed, and modified. RStudio offers aids such as syntax highlighting, automatic indentation, and autocompletion, although it takes some time to understand these functionalities and incorporate them into daily practice. These features not only reduce technical errors but also facilitate code readability. Colors, indents, and suggestions make structures visible that would otherwise remain hidden in a uniform mass of characters. The IDE, in this sense, acts as a mediator that makes the language more legible. A script is, ultimately, a text.\nWe can think of the script editor as a word processor, like Word or GoogleDocs. The editor will mark with a red cross when something alters R’s syntax (just as word processors underline words with spelling errors). Paying attention to errors is a very effective way to understand how the syntax of a programming language works.\nIn the console, we can execute functions directly, although they will not be saved unless we write them in the script (and this can cause problems if we lose track of our code). Working with the console can be useful when learning, because it enables constant testing, error, and reformulation, but the fact that the code and its results appear in the same place can hinder the clarity of our progress. The IDE does not force a linear path but accompanies back-and-forth processes between hypotheses, code, and results.\nAnother relevant aspect of RStudio is its integration with projects. Working on a project (a post on this is coming!) involves organizing files, data, scripts, and results within a coherent structure. This practice, which may seem excessive at first, becomes fundamental when analyses grow in complexity or when they are resumed after some time. The IDE facilitates this organization and, in doing so, promotes a more reflective and documented way of working. Code ceases to be something ephemeral that is executed once and lost, to become a record of the analytical process. This also fosters reproducibility, a crucial aspect of scientific practice.\nRStudio also includes direct access to documentation, meaning the description and usage guide for functions and packages. When writing a function, it’s possible to quickly consult what it does, what arguments it expects, and what it returns. For example, executing ?summary in the console will yield the specific usage guide for that function. This proximity to help reinforces the idea that programming involves reading as much as writing. Functions are not used blindly; instead, they are interpreted, compared, and chosen based on what one wants to do with the data. No one knows everything by heart; instead, we constantly refer to the materials produced by those who created these functions.\nIt is important to emphasize that learning to use an IDE is not a prerequisite for “knowing how to program,” but it is part of the technical literacy that accompanies language learning. The IDE shapes habits, reading modes, and ways of organizing work. In that sense, it is not neutral. RStudio, in particular, is designed for analytical practices typical of academic work: data exploration, reproducible writing, combining code and text, and producing plots and reports.\nThinking of the IDE as a mere technical support can lead to underestimating it. In reality, it functions as a workspace that embodies a certain conception of programming. For those starting with R from the social sciences and humanities, understanding what an IDE is and how it organizes the programming experience helps dismantle the idea that code is opaque or inaccessible. The environment is also read, interpreted, and learned to be used in a situated manner.\nIn the upcoming texts of this series, we will revisit RStudio in greater detail, observing how certain interface decisions interact with concrete analytical practices. Understanding the environment is a key step to start thinking of code as a language one works with, and not just as a tool that is executed."
  },
  {
    "objectID": "posts/what-is-data-science.html",
    "href": "posts/what-is-data-science.html",
    "title": "What is Data Science and Why All the Interest?",
    "section": "",
    "text": "In recent years, the expression data science has become ubiquitous. It appears in job offers, academic programs, research projects, and public debates. Sometimes it is presented as a new and homogeneous discipline; other times, as a diffuse set of techniques. To understand what it is about, it is useful to place it historically, review its links with statistics and big data, and consider why it is particularly attractive to those coming from analytical traditions linked to the social sciences.\n\nAn origin linked to concrete problems\nData science does not emerge from a single field or at a precise moment. It consolidates from the convergence of existing practices: statistical analysis, programming, database management, and working with large volumes of information. By the mid-20th century, applied statistics already played a central role in scientific research and decision-making. Later, with the expansion of computing and digital storage, it became possible to work with increasingly larger and more complex datasets.\nThe term data science began to circulate more forcefully towards the late nineties and early two thousands, when it became evident that the problems were no longer just about calculating indicators, but about organizing, cleaning, transforming, and interpreting heterogeneous data. Data science thus configures itself as a practical response to a recurring question: how to produce knowledge from data in contexts where the volume, variety, and velocity of information challenge traditional approaches.\n\nData science and statistics\nThe relationship between data science and statistics is close, though not always obvious. Many of data science’s central tools, such as estimation, inference, or modeling, come directly from statistics. However, data science broadens the focus. In addition to analyzing already prepared data, it deals with the entire process: from obtaining information to communicating results.\nIn this sense, programming plays a key role. Not only as a means to execute calculations, but as a way to describe procedures explicitly and reproducibly. Code allows documenting decisions, repeating analyses, and adjusting intermediate steps. Statistics provides the conceptual frameworks for interpreting results, while programming articulates these frameworks with concrete data and complex workflows.\n\n\nThe link with big data\nBig data often appears associated with data science, although they are not synonyms. Big data refers, in general terms, to large or highly complex datasets that require specific infrastructures for their storage and processing. Data science can work with big data, but also with small databases, surveys, administrative records, or textual corpora.\nWhat they share is a common concern: how to transform data into meaningful information. In many cases, the challenge is not the quantity of data, but its quality, structure, and context of production. From this perspective, data science is not defined solely by volume, but by an approach to analysis that integrates technique, interpretation, and decision-making.\n\n\nWhy it sparks interest in the social sciences\nThe growing interest of people trained in social disciplines in data science has several reasons. Firstly, many contemporary research projects work with digital data: online surveys, administrative databases, social networks, textual archives, or interaction logs. These materials require tools that allow for systematic exploration and analysis.\nSecondly, data science proposes a way of working that dialogues well with classic concerns of social analysis. The need to make assumptions explicit, document procedures, and reflect on the categories used finds an ally in the use of code and reproducible workflows. The analysis leaves traces that can be read, discussed, and reviewed.\nFurthermore, data science brings to the forefront questions about the power of data, biases, representativeness, and the social uses of information. These questions are not foreign to the critical traditions of the social sciences. On the contrary, they offer a space where technical tools and conceptual reflection meet.\n\n\nA situated practice\nMore than a closed discipline, data science can be understood as a situated practice. Its tools adapt to specific problems and concrete contexts of research, work, or intervention. Learning data science involves learning to formulate questions, evaluate sources, make methodological decisions, and communicate results responsibly.\nFrom this perspective, programming, analyzing, and visualizing data are not ends in themselves. They are means to construct knowledge in dialogue with theoretical frameworks, substantive questions, and material conditions of production. Data science thus becomes a fertile space for those who seek to articulate technique and reflection, without losing sight of the fact that data are always anchored in social practices."
  },
  {
    "objectID": "posts/r-base-tidyverse.html",
    "href": "posts/r-base-tidyverse.html",
    "title": "R base, packages, and Tidyverse: what we talk about when we use R",
    "section": "",
    "text": "When we start working with R, one of the first expressions that appears is “R base.” No, just kidding: when we start working with R, we have no idea what we’re doing. We copy code from the courses we’re taking or from the books or tutorials we’re following. It’s possible that these materials contain information about the source of those functions, but as learners, our working memory is a bit saturated, and we’ve probably not paid attention to it.\nAs we incorporate concepts and automate our workflow, we can start paying attention to the theory. And we see these concepts emerge: R base, packages, libraries, Tidyverse. These words circulate naturally in tutorials, classes, and forums, although their meaning and how they relate to each other are not always explicitly stated. However, understanding this architecture is key to being able to read code with greater autonomy and to make informed decisions about how to work with data.\nR base refers to the set of functionalities included when R is installed. It includes the language itself, a wide range of fundamental functions, and some basic packages that are loaded automatically. Arithmetic operations, object creation, basic handling of vectors and data frames, functions like mean(), sum(), plot(), or summary() are part of this core. R base defines the minimal grammar of the language and establishes the rules that allow everything else to function.\nWorking solely with R base is possible, and in fact, for many years, it was the standard way to use R. However, that core is designed to be extended. R was designed from the beginning as a language1, capable of incorporating new functionalities without modifying its central structure. This extensibility is materialized through packages.\nA package is an organized collection of functions, data, and documentation that can be downloaded and incorporated into an R session. Each package addresses a specific need: particular statistical analysis, visualization, text manipulation, survey work, advanced models, or specific data formats. Technically, installing a package means downloading it to your computer; using it means loading it into the active session. Conceptually, using a package implies adopting a particular way of solving analytical problems. There are packages of all types and colors: some focus on functions, others on data. Given that many different packages perform the same actions, there has recently been much emphasis on the importance of citing packages used in data analysis, which has led to the emergence, not without some irony, of packages that facilitate package citation.\nPackages can be thought of as crystallizations of research practices. Whoever develops a package makes decisions about which operations to facilitate, how to name them, which data structures to prioritize, and which assumptions to take for granted. When we use a function from a package, we are not only reusing code but also incorporating a certain way of thinking about analysis. While it is difficult to know a package thoroughly from which we are taking a function, it is always good practice to read its documentation.\nAt this point, another source of confusion often appears: the term “library.” In R, “library” is used to refer to the location where packages are installed on the system, and also, by extension, to the act of loading them using the library() function. In everyday practice, speaking of packages and libraries is often interchangeable, although from a technical standpoint, they are not exactly the same. The important thing is to understand that R base is extended through packages that are loaded according to the needs of the analysis.\nWithin this universe of packages, there is one that occupies a particular place: the Tidyverse. The Tidyverse is not a single package, but a collection of packages designed to work coherently with each other. It includes widely used tools for data manipulation, visualization, and file import, such as dplyr, ggplot2, tidyr, readr, and stringr, among others. All share a common philosophy and a relatively consistent syntax, found in the book R for Data Science by Hadley Wickham and Garrett Grolemund.\nThe central proposal of the Tidyverse is to organize data work based on clear principles. One of the best known is the concept of “tidy data,” where each variable occupies a column, each observation a row, and each type of analytical unit a table.\nThis principle, which may seem purely technical, has important analytical implications because it forces one to explicitly state what is considered a variable, what counts as an observation, and how data is structured. This can change not only between datasets but also between analyses and even between functions. For example, a function that performs a statistical analysis comparing groups might take the grouping variable from a single column or might require each group to have its own column.\nAnother distinctive feature of the Tidyverse is its emphasis on code readability. Functions often have verbal names, arguments prioritize clarity, and the use of the %&gt;% operator suggests a sequential reading of operations. For those coming from traditions where text interpretation and procedure explication are central, this orientation is particularly appealing. The code is presented as a sequence of transformations that can be read almost like a narrative of the analysis (or like a cooking recipe!)2.\nThis does not mean that the Tidyverse replaces R base. In fact, it constantly relies on it. Many Tidyverse functions wrap or reorganize existing functionalities in R base, offering a different interface. Choosing to work with R base, with specific packages, or with the Tidyverse is not a matter of correctness, but of approach. Each option implies adopting certain conventions and foregoing others. It also has to do with knowing the audience for our code: certain packages tend to be more famous in specific fields, and that can prioritize our decision to use them over less known alternatives. Ultimately, it’s about using the language in a way that will facilitate its understanding.\nUnderstanding these differences allows us to move beyond a purely instrumental logic. Using R is not just about knowing which command to execute, but about understanding what conceptual framework we are employing when doing so. R base, packages, and the Tidyverse form layers of the same language, which combine in various ways depending on the research problem, the type of data, and the questions one wants to ask.\nIn the next texts in this series, we will revisit these tools with concrete examples. The idea will not be to learn lists of functions, but to develop criteria for reading code, recognizing styles, and consciously choosing how to work with R in situated research contexts."
  },
  {
    "objectID": "posts/r-base-tidyverse.html#footnotes",
    "href": "posts/r-base-tidyverse.html#footnotes",
    "title": "R base, packages, and Tidyverse: what we talk about when we use R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAgain, metaphors appear that bring programming language closer to other disciplines. In this case, I’m thinking of Jerry Fodor’s the modular conception of the mind.↩︎\nA clarification is needed here. The famous %&gt;% operator (pipe) was for a long time synonymous with the Tidyverse, although it comes from the specific magrittr package. However, R version 4.1.0, released in May 2021, incorporated a similar operator, |&gt;, meaning that function chaining, like that seen in that charming GIF, is no longer exclusive to the Tidyverse. Strictly speaking, both operators have different functionalities, so massively replacing the %&gt;% operator with |&gt; in our code can lead to errors. The differences might be too technical for someone just starting, but if you’re interested, you can read more here.↩︎"
  },
  {
    "objectID": "posts/projects-version-control-github.html",
    "href": "posts/projects-version-control-github.html",
    "title": "RStudio Projects and Version Control: Working Without Getting Lost in Files",
    "section": "",
    "text": "Those who start working with data usually do so with loose files. A script called final_analysis.R, then another final_analysis_v2.R, then real_final_analysis.R. As work progresses, copies appear on different devices, versions sent by email, and duplicated folders. If several people are also involved, the situation quickly becomes confusing: it’s not clear which is the most updated file, what changes each person made, or which version was used to obtain a result.\nA common response to this problem is to move everything to the cloud. Services like Google Drive or Dropbox allow files to be available from different locations and devices. This solves part of the problem, because it reduces the circulation of disconnected copies. However, an important question remains open: how to record changes over time, go back if something goes wrong, or understand what was modified between one version and another of the same file.\nThat’s where version control comes in, and particularly platforms like GitHub1. Version control allows you to save the history of a project, record each change in an organized way, and work collaboratively without stepping on each other’s work. GitHub adds a cloud infrastructure that facilitates sharing projects, collaborating, and maintaining a clear reference of which version is current. Instead of exchanging finished files, you work on the same project, with an explicit record of its evolution.\nIn short, the way Git works is as follows: we have a repository (we can think of it as a folder hosted in the cloud) from which we download a copy to our computer. We make changes to that repository, but as long as we don’t do anything else, the version in the cloud (on GitHub) will remain as it was before. Once we finish working or reach a satisfactory result, we “upload” those changes to that repository and the one in the cloud is updated (and both the repository in the cloud and the one we have on our computer will be identical).\nDifferent people can work with the same repository and “upload” their changes, as long as those changes do not conflict. When this happens (and it will happen, comrades, inevitably), Git allows us to know where that conflict occurs, which versions conflicted (“up to version 4 everything was fine, in Pepito’s version 5 everything went wrong”) and gives us the possibility to revert changes to a previous version (or forcibly accept the changes). The good thing is that Git keeps a record of everything, which can get us out of trouble more than once."
  },
  {
    "objectID": "posts/projects-version-control-github.html#footnotes",
    "href": "posts/projects-version-control-github.html#footnotes",
    "title": "RStudio Projects and Version Control: Working Without Getting Lost in Files",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile we will talk about GitHub here, strictly speaking, one thing is Git and another is GitHub. Git is the version control system that is installed locally and is responsible for recording changes in files, something like the technology that allows tracking changes. GitHub, GitLab, and similar platforms are services that host Git repositories in the cloud and add tools for sharing, collaborating, and managing projects.↩︎"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "What is object-oriented programming and why is R considered an object-oriented language?\n\n\nWhat does it mean for a language to be object-oriented? This post introduces the concept of object-oriented programming and explains how R implements this logic through classes, objects, and methods in daily data work.\n\n\n\n\n\nJan 29, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Data Science and Why All the Interest?\n\n\nWhat is data science and where does it come from? This post explores its origin, its link to statistics and big data, and analyzes why more and more people are approaching this field from diverse analytical traditions.\n\n\n\n\n\nJan 28, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\n6 Essential Books for Learning Data Science\n\n\nFive books available online for free for an introduction and in-depth study of data science, from fundamentals and communication to programming with R and applied statistics.\n\n\n\n\n\nJan 27, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nGrammar of Graphics\n\n\nWhat do statistical graphics have in common? This post introduces the idea of the grammar of graphics and shows how ggplot2 allows thinking of visualization as a language composed of explicit analytical decisions.\n\n\n\n\n\nJan 26, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Projects and Version Control: Working Without Getting Lost in Files\n\n\nWorking with loose files and duplicated versions leads to confusion, especially in collaborative projects. This post introduces the use of RStudio projects combined with version control in GitHub and proposes a more organized and reproducible workflow.\n\n\n\n\n\nJan 24, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility: why it matters and what it has to do with good programming\n\n\nWhat does it mean for an analysis to be reproducible and why does it matter? This post explains the idea of reproducibility in science and shows how good programming practices help support clear, verifiable, and easy-to-resume analyses.\n\n\n\n\n\nJan 22, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with projects in RStudio: organizing your work from the start\n\n\nWhat does it mean to work with projects in RStudio and why is it advisable to do so from the beginning? This post explains what projects are for and shows step-by-step how to create and use one in practice.\n\n\n\n\n\nJan 22, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with R: Installation and First Steps\n\n\nHow to start with R from scratch? This post explains step-by-step how to install R and RStudio, how to install and load the Tidyverse, and what to check to ensure everything is working correctly.\n\n\n\n\n\nJan 22, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nConsole and Script: Two Ways to Write Code in R\n\n\nConsole and script serve different functions in R. This post clearly explains what each is for, when to use them, and how they complement each other in daily work.\n\n\n\n\n\nJan 21, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nR base, packages, and Tidyverse: what we talk about when we use R\n\n\nWhat is R base, and what changes when we use packages? This post explains R’s modular structure, introduces the role of packages, and dedicates a section to the Tidyverse as a set of tools with its own philosophy for working with data.\n\n\n\n\n\nJan 20, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nR for the Humanities: Why Learn to Program?\n\n\nWhy learn to program from the social sciences and humanities? This post proposes thinking about R as a language for analysis and writing, shows simple examples of commented code, and presents programming as a situated practice of knowledge production.\n\n\n\n\n\nJan 18, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nR: A Language Born to Think with Data\n\n\nWhat does programming have to do with reading, interpreting, and producing knowledge in social sciences and humanities? This text proposes an entry into R from its history and academic uses, showing how its design, its function-based logic, and its package ecosystem interact with familiar analytical practices. More than a technical introduction, it is an invitation to read code as language and to think of programming as a situated research practice.\n\n\n\n\n\nJan 17, 2026\n\n\nAtelier de Código\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrated Development Environments (or IDEs, for friends)\n\n\nWhat is an IDE and why does it matter when starting to program in R? This text introduces the concept of an integrated development environment, explores common IDEs, and focuses on RStudio as a workspace designed for academic analysis. An invitation to understand the environment as a mediator between code, data, and research practices.\n\n\n\n\n\nJan 17, 2026\n\n\nAtelier de Código\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/reproducibility.html",
    "href": "posts/reproducibility.html",
    "title": "Reproducibility: why it matters and what it has to do with good programming",
    "section": "",
    "text": "In many research areas, results are not produced all at once. They are built from data, analytical decisions, adjustments, corrections, and backtracking. In this process, a question arises sooner or later: could another person obtain the same results by following the same steps? This possibility is often called reproducibility.\nReproducibility refers to the ability to repeat an analysis and arrive at the same results using the same data and the same procedures. It does not imply that the results are universal or definitive, but rather that the path that led to them is clear and verifiable. In practical terms, a reproducible analysis allows one to understand what was done, how it was done, and in what order.\nLack of reproducibility is rarely due to malicious intent (though sometimes it happens). Rather, it is often associated with common but unsystematic practices: data files modified without a log, analyses partially done in the console, intermediate steps not saved, results copied and pasted into final documents. Over time, even the person who performed the analysis can lose the ability to reconstruct it.\n\n\n\nWhen we look for the culprit as to why our code no longer works just a few months after we wrote it.\n\n\nAt this point, programming plays a central role. Working with code allows for an explicit record of each decision. A script can show how data was loaded, what transformations were applied, what models were fitted, and how results were produced, both tables and graphs. This record does not depend on memory or subsequent explanations. It is written and can be read, either in plain text using an RMarkdown or Quarto file, or in the comments within the code itself.\nGood programming practices reinforce this logic. Writing code in scripts instead of just in the console allows the entire process to be preserved. Using clear names for objects and functions facilitates readability. Adding comments helps understand why a certain decision was made and not another. Organizing work into projects keeps data, code, and results together. All these practices aim at the same thing: that the analysis can be resumed and understood.\nReproducibility also benefits from the separation between raw data and processed data. Maintaining an original version of the data and performing all transformations via code prevents hard-to-detect errors. If something changes in the input data (for example, if you added subjects to your data collection), the analysis can be rerun without needing to manually redo steps.\nAnother key aspect is automation. When results are generated from code, there’s no need to copy values from one place to another. Graphs, tables, and statistics are produced directly from the scripts. This reduces errors and ensures that the results are aligned with the data and the current analytical decisions.\nTools like RStudio facilitate this approach. Working with projects, using scripts, and the ability to integrate code and text into reproducible documents like RMarkdown allow for building analyses that run from beginning to end. The result is more transparent work, both for the person performing it and for the person reading it.\nReproducibility is not an abstract goal or an external requirement. It has very concrete effects on daily work. It saves time when something needs to be corrected, allows analyses to be resumed after weeks or months, and facilitates exchange with other people. Even when the analysis will not be shared publicly, working reproducibly improves the quality of the process.\nGood programming alone does not guarantee reproducibility, but it creates the conditions for it to be possible. Writing legible, organized, and documented code transforms the analysis into an object that can be reviewed, discussed, and improved. In that sense, good programming practices are as much a part of scientific work as formulating questions or interpreting results.\nIn the upcoming posts in this series, we will continue to delve into concrete tools that help sustain this way of working, from code organization to the use of version control."
  },
  {
    "objectID": "posts/r-humanities-learning.html",
    "href": "posts/r-humanities-learning.html",
    "title": "R for the Humanities: Why Learn to Program?",
    "section": "",
    "text": "Learning to program is often presented as a technical skill, associated with a technology-focused job market. For those of us who come from the social sciences and humanities, that world can feel alien. Our daily work has a different materiality (texts?) and is shaped by questions about language, interpretation, category construction, and situated knowledge production. However, increasingly, technological tools are emerging that allow these two worlds to be intertwined, and more and more people from linguistics, sociology, or political science are working with code and data science.\nWe often stop to think about what programming brings to those of us who work in these areas, but what can we bring to that world? What new perspectives can we offer on technology? There’s one that seems central to us, which we’ve mentioned several times because it’s at the core of our experience: programming is writing. Beyond the obvious associations between a “natural language” and a “programming language,” we fundamentally believe there’s a textual dimension shared by both texts and scripts. In this context, we propose to view programming not so much as an instrumental skill, but rather as a practice of writing and analysis that dialogues with already familiar concerns.\nProgramming involves writing instructions in a formal language, but also reading, interpreting, and revising them. An script in R is not just a sequence of commands that “works” or “doesn’t work”: it’s a text that condenses analytical decisions, theoretical assumptions, and methodological choices. Where do we start? How much do we focus on descriptive analysis versus inferential analysis? Do we write alone or do we expect someone else to read us? Do we comment on our text?\nJust as we analyze an academic text by asking what concepts it uses, what it leaves out, or how it organizes its argument, code can also be read critically. Learning to program in R, then, opens up the possibility of thinking with data in an explicit and revisable way.\nR is especially interesting for this cross-section because it was born and developed in academia. Its widespread use in statistics, linguistics, sociology, psychology, and education sciences is no coincidence. The logic of the language favors exploratory and reflective work with data, where each analysis step can be recorded. Furthermore, its package ecosystem reflects specific disciplinary debates and traditions, which materialize in functions, arguments, and data structures. Paradoxically, many people see the R vs Python debate as an academia vs. industry debate (mostly by those from industry who have negative biases towards academic work).\nSomething of this is seen in course learning materials: the vast majority focus on instructions on how to perform a specific task, but rarely are there explanations about the intrinsic functioning of the language. In fact, the linguistic aspect of a programming language is rarely highlighted, despite inheriting attributes such as syntax) or semantics. Those of us who come from linguistics and have an ingrained perspective on the formal or combinatorial aspects of structures have an advantage that is not usually mentioned. Rather, the opposite: those from the social sciences and humanities often feel that their way of thinking is different from what is needed to enter the world of technology (we discussed this at #LatinR2024, in Spanish).\nFrom our perspective, learning to program does not mean abandoning practices specific to other areas. On the contrary, it means extending them to a new type of text. Code becomes a space where theory, method, and data are articulated. Tools like RStudio, and practices such as using reproducible scripts or documents in R Markdown, reinforce this idea by integrating code, text, and results into a single medium.\nIn closing this reflection, it’s worth insisting on a central idea: programming is a situated practice. It is learned in specific contexts, to answer concrete questions, with determined disciplinary traditions. Learning R from the humanities and social sciences does not imply simply adopting an alien language, but rather appropriating it, reading it critically, and using it to think about one’s own problems. Programming, in this sense, is another way of writing, analyzing, and producing knowledge."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "You can reach us at hola@atelierdecodigo.com"
  },
  {
    "objectID": "posts/r-programming-language.html",
    "href": "posts/r-programming-language.html",
    "title": "R: A Language Born to Think with Data",
    "section": "",
    "text": "When we, from the social sciences and humanities, first approach a programming language, an ambiguous feeling often arises: curiosity, interest, and, at the same time, a certain caution. Programming is frequently associated with a technical practice, distant from reading, discursive analysis, or theoretical reflection. However, we can choose a different entry point to learn R. Its history, its uses, and the community that sustains it make it a language particularly close to an analytical perspective on data and on the conditions of knowledge production.\nR originated in the field of academic statistics. In the early 1990s, Ross Ihaka and Robert Gentleman, statisticians at the University of Auckland, developed R as a free re-implementation of the S language, used since the 1970s in statistical research. From that initial moment, R was conceived as a tool for exploring data, testing models, and supporting scientific research processes. This origin helps to understand many of its current characteristics and also the practices that have consolidated around its use.\nOne of R’s distinctive features is its orientation towards interactive analysis. The code executes progressively and allows for exploratory work with data: trying an operation, observing the result, adjusting the instruction, and re-evaluating. For those accustomed to building analyses iteratively, based on provisional hypotheses and constant reformulations, this mode of work is particularly familiar. Programming in R usually involves sustaining a process of controlled exploration, rather than following a completely predefined path.\nFrom a technical point of view, R relies heavily on the use of functions. Most operations are performed by applying functions to objects (in fact, R is an object-oriented language, just like Python). A function can be understood as a formalized operation that receives one or more arguments, executes a series of internal steps, and returns a result. Calculating an average, transforming a variable, fitting a model, or producing a graph are examples of tasks performed using functions. Reading code in R involves identifying which functions appear, what arguments they receive, and what type of object they generate as output.\nThis emphasis on functions favors a careful reading of code as if it were a text. Each function call presents a relatively stable structure: a name, parentheses, and arguments that can be explicitly named or implicitly defined by default values. For those with experience in text analysis, this syntax can be approached as a system of markers that guides interpretation. Understanding why a function produces a certain result often requires pausing on these details and on the assumptions that the function incorporates.\nOver time, the uses of R expanded considerably. Currently, it is used for statistical analysis, modeling, data visualization, text analysis, social network studies, working with spatial data, and experiments in psychology and linguistics, among many other fields. This expansion is largely explained by the package system. A package is an organized collection of functions, data, and documentation that extends the language’s capabilities. R thus functions as a constantly growing environment, fed by contributions from different academic communities. For the social sciences and humanities, this feature is particularly relevant. Many of the tools available in R were developed by researchers working on problems similar to ours. Packages oriented towards the analysis of surveys, texts, linguistic corpora, or longitudinal data incorporate specific theoretical and methodological decisions. By reading the code that uses these packages, one also accesses these decisions: how a unit of analysis is defined, which operations are considered relevant, what assumptions are adopted about the data.\nIn this sense, R can be thought of as a language in which particular ways of thinking and researching are inscribed. The existence of object classes, the representation of models as examinable objects, or the construction of layered graphics organize the relationship between the analyst and the analyzed phenomenon. Learning R implies learning to recognize and work with these mediations.\nFor this reason, in a series aimed at those beginning programming from the social sciences and humanities, it is productive to present R as a language with a history, conventions, and communities of use. Reading code in R, even when it initially takes time and effort, is part of a technical literacy that dialogues with habitual practices of reading and interpretation.\nIn the next texts in this series, we will introduce concrete code snippets. The goal will be to learn to read them, decompose them, and understand what they are doing and what they are saying. Programming in R, from within our disciplines, implies incorporating a new form of writing and of situated knowledge production."
  },
  {
    "objectID": "posts/console-script-writing-code.html",
    "href": "posts/console-script-writing-code.html",
    "title": "Console and Script: Two Ways to Write Code in R",
    "section": "",
    "text": "When R or RStudio is opened for the first time, the console is usually the starting point. It’s the window where the &gt; symbol appears and where instructions can be written directly. You write a line of code, press Enter, and R responds. That response can be a number, a table, a message, or a plot. The console basically serves to tell R what to do and see the result immediately.\n\nThe console is useful for quick actions. For example, calculating an average, viewing the content of an object, or testing if a function does what we expect. It’s also common to use it for small trials while learning. The problem is that what is written in the console is not saved in an organized way. If R is closed or the session is restarted, that history is lost. Even with the session open, reviewing what was done a while ago can be confusing.\nThe script serves another function. A script is a file where code is written to be saved. In RStudio, it usually has the .R extension and opens in the editor pane (you can go to File, New File, R Script or click on the icon with a green cross). There, you can write many lines of code, add comments, and organize the analysis step by step. Writing in a script does not execute the code automatically. Each line or block is run only when indicated, either with the button that says “Run”, or with Control+Enter.\n\nThis difference is important. The script allows separating two moments: writing and executing. First, the code is written calmly. Then, it is decided what to execute and when. This makes the work more organized and easier to pick up again. If the file is reopened days or weeks later, the code is still there, in the order it was conceived.\nA very common use is to combine both spaces. The console is used to quickly test something. If that code proves useful, it is copied or written directly into the script. In this way, the script becomes the main record of the work, while the console functions as a testing ground.\nRStudio facilitates this way of working. From the script, code can be executed, and the result appears in the console, but the code remains saved in the script file. This dynamic is especially useful when the analysis has multiple steps or when the work needs to be shared with other people.\nIn practical terms, a good initial rule is this: the console serves for testing and exploring; the script serves for saving and organizing. There’s no need to choose one over the other exclusively. The important thing is to understand what each space contributes and to use it consciously.\nLearning to write in scripts from the beginning saves time later on. It allows for easy error correction, repeating analyses without starting from scratch, and better understanding what was done at each stage. For beginners, this distinction often marks a before and after in how they work with R."
  },
  {
    "objectID": "posts/books-learning-data-science.html",
    "href": "posts/books-learning-data-science.html",
    "title": "6 Essential Books for Learning Data Science",
    "section": "",
    "text": "Delving into data science goes beyond learning syntax or executing commands. Reading books that articulate concepts, techniques, and reflections helps to understand the field as a whole. Fortunately, there are texts freely available on the web that cover everything from fundamentals to concrete applications. Below I present five recommended, open-access books. Some are in English, others in Spanish.\n\n1. R for Data Science (Hadley Wickham, Garrett Grolemund, and Mine Çetinkaya-Rundel)\nThis book is the absolute foundation for programming in R in the world of data science. It covers topics such as data cleaning and visualization, among others. It is designed for beginners but with a level of detail that allows for continuous learning. It has its Spanish version here.\n\n\n2. Telling Stories with Data (Rohan Alexander)\nFocused on statistical communication and analysis narrative, this book covers everything from data collection and cleaning to results presentation. It includes code examples and activities that reinforce the understanding of each technique. Its focus on how to translate data into clear conclusions makes it especially useful for those working with interpretive analysis.\n\n\n3. Libro Vivo de Ciencia de Datos (Pablo Casas)\nThis book in Spanish presents an intuitive introduction to data science and machine learning, with a didactic approach. It offers a practical vision of how to think and work with data from initial levels.\n\n\n4. Deep R Programming (Marek Gagolewski)\nAlthough more technical, this free resource offers a deep introduction to the R language from a data science perspective. The book covers data transformations, numerical computation, functional programming, and advanced structures, with practical examples and exercises.\n\n\n5. OpenIntro Statistics (David Diez, Christopher Barr, and Mine Çetinkaya-Rundel)\nAlthough this text is a statistics book, it is essential reading for data science. Statistics is a pillar of data analysis, and this book guides the reader from basic concepts to applied techniques.\n\n\n6. Learning Statistics with R (Danielle Navarro)\nThis book in English compiles the materials from an introductory statistics course that the author taught at the University of Adelaide, with examples in R. It condenses not only the theory but also the practice for conducting real and informed data analysis. Furthermore, Danielle is a very active person in the R community, and her contributions are truly fundamental.\n\n\nHow to Use These Books in Your Learning\nThese texts serve different purposes: some introduce general concepts, others delve into specific tools, and others integrate programming and analysis. An effective reading strategy can combine a more conceptual book with practical materials that include code exercises and real-world cases.\nIf you are just starting, it can be useful to begin with introductory chapters (for example, data science fundamentals and results communication) before moving on to more technical texts or those focused on specific languages like R."
  },
  {
    "objectID": "posts/projects-rstudio.html",
    "href": "posts/projects-rstudio.html",
    "title": "Working with projects in RStudio: organizing your work from the start",
    "section": "",
    "text": "When you start using R, it’s common to work with loose files: a script here, a data file there, results saved on the desktop or in unclear folders. This mode of work often functions at first, but it becomes fragile as the analysis grows, as time passes, or when another person needs to understand what was done (or you, in the future!). At that point, a key RStudio tool appears: projects.\nAn RStudio project is a way to organize your work within a main folder. Everything that belongs to a specific analysis or project lives within that space: scripts, data, plots, tables, and documents. When you open a project, RStudio knows where it stands and always works from that location. This avoids one of the most frequent problems when starting out: errors with absolute or relative paths, and files that “cannot be found”.\nFrom a conceptual point of view, working with projects helps to make the work structure explicit. The analysis stops being a succession of dispersed commands and gains a clear framework. Each file serves a purpose, and its location makes sense. Furthermore, the project saves information about the working environment, making it easier to resume analysis after a period without having to reconstruct everything from scratch.\nProjects also promote reproducibility. If everything is within the same folder, it’s easier to move the work to another computer or share it, for example, through a Github repository. There’s no need to rewrite absolute paths or adjust settings every time. RStudio takes care of opening the project and getting everything ready to continue.\n\nHow to create an RStudio project step-by-step\n\nOpen RStudio.\nIn the top menu, click on File and then on New Project….\nRStudio will offer three options. To start, the simplest one is New Directory.\nChoose New Project.\nSpecify the project name and the location where you want the folder to be created.\nClick on Create Project.\n\nRStudio will create a folder with that name and automatically open the project. From that moment on, every time you open that project file, identified by the .Rproj extension, RStudio will work from that folder. You’ll see it in the upper right corner, next to a light blue cube with an “R”:\n\n\n\nWhat changes when working within a project\nOnce the project is created, it’s advisable to adopt some simple practices. Save scripts within the project folder, for example in a subfolder called scripts. Place data in another folder, such as data. Save plots or tables in specific folders. This organization is not mandatory, but it helps make the work more readable.\nAnother important change is the use of relative paths. Within a project, when referring to a file, it is named in relation to the project folder. For example, data/survey.csv. This works as long as the project is open and prevents errors when the analysis is moved.\nRStudio also displays the name of the active project in the upper right corner. This visual detail serves as a constant reminder of the context you are working in. Opening a script without opening the project can cause confusion, so it’s advisable to get used to always starting your work from the .Rproj file.\nFinally: a personal recommendation. Before finishing work or closing the file, try restarting your RStudio session (in the top bar, Session and then Restart R), and try running all the code from scratch. If everything goes well, then your code is clean and ready to be saved. But if you modified something (for example, in the console) that broke the continuity of the code, an error will pop up. And believe me, it’s better to correct it right away than the next day (or months or years later…).\n\n\nProjects as a work habit\nWorking with projects doesn’t require advanced knowledge or change the way you write code. It primarily changes how you organize it. Incorporating this habit from the start saves time, reduces errors, and makes the analysis easier to understand, even for the person who created it.\nIn the next post in this series, we will go a step further and explain how to connect an RStudio project with a version control system. This allows you to track changes, revert to previous states of your work, and collaborate in a more organized way."
  },
  {
    "objectID": "posts/install-r-rstudio.html",
    "href": "posts/install-r-rstudio.html",
    "title": "Getting Started with R: Installation and First Steps",
    "section": "",
    "text": "Starting with R often raises very specific questions. What to install, in what order, what each thing is for. Before writing a single line of code, it’s worth clarifying this point, because R works with several pieces that combine with each other. The good news is that the installation process is simple and only needs to be done once.\nThe first step is to install R. R is the programming language itself. To do this, go to the official project site CRAN, and choose the installer file that corresponds to your operating system (Windows, macOS, or Linux). By clicking, you will access the specific instructions for each case.\n\n\n\nInstall R\n\n\nTechnically, we could use R directly from the software we just installed, on Windows and macOS. However, it is not a very user-friendly graphical interface, so most people use other environments like RStudio, VSCode, or Positron. If we are just starting, it is preferable to use RStudio, because most tutorials will show images of that environment.\nAs we mentioned in a previous post, RStudio is an independent program that is installed separately. It is a graphical interface (technically, an integrated development environment) that facilitates the interaction between the user and the R programming language. To install it, you have to go to the RStudio site and download the free version. As seen in the image, we can also install R from here, but if we have already installed it from CRAN, there is no need to install it again.\n\n\n\nInstall RStudio.\n\n\nWhen opening RStudio for the first time, it automatically detects the R installation and is ready to use. From that moment on, RStudio becomes the main workspace.\nWith R and RStudio installed, it is now possible to write and execute code. The next usual step is to install packages, which are add-ons that extend R’s capabilities. One of the most used is the Tidyverse, a set of packages designed to work with data in an organized and readable way (you can read more about what packages are here).\nInstalling a package in R is done by typing an instruction in the console. In the case of the Tidyverse, the command is:\n\ninstall.packages(\"tidyverse\")\n\nOnce we write the code, we must execute it (control+Enter or the “Run” icon). This step requires an internet connection and may take a few minutes, because the Tidyverse includes several packages. Many colored messages will appear in the console: this is normal (unless something says “Warning” or “Error”). The installation is done only once. Once installed, to use it in a work session, you have to load it with:\n\nlibrary(tidyverse)\n\nWarning: package 'tibble' was built under R version 4.4.1\n\n\nWarning: package 'purrr' was built under R version 4.4.1\n\n\nWarning: package 'stringr' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.6.0\n✔ ggplot2   3.5.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThis difference between installing and loading often causes confusion at first. Installing downloads the package to the computer. Loading makes it available in the current R session. Every time a new session is started and you want to use the Tidyverse, you have to run library(tidyverse) again.\nAfter loading it, R shows some messages in the console indicating which packages were activated. From that moment on, functions like ggplot, mutate, or filter become available. There’s no need to understand them all immediately. The important thing is to know that they are part of the same ecosystem and share a common logic.\nA practical tip for beginners is to verify that everything is working. For example, write a simple line like:\n\nggplot(mtcars, aes(x = mpg, y = wt)) + geom_point()\n\n\n\n\n\n\n\n\nIf a plot appears, the installation was successful. It doesn’t matter if the code is not yet understood. That will be worked on later.\nInstalling R, RStudio, and the Tidyverse marks the beginning of the work. From there, learning R involves familiarizing oneself with the environment, with writing scripts, and with reading code. Having these tools well installed and working avoids many unnecessary problems and allows you to focus on what’s most important: understanding what the code does and how to use it to analyze data.\nIn the next posts of this series, we will break down these pieces in more detail, so that the initial step transforms into a solid working foundation."
  },
  {
    "objectID": "posts/object-oriented-programming.html",
    "href": "posts/object-oriented-programming.html",
    "title": "What is object-oriented programming and why is R considered an object-oriented language?",
    "section": "",
    "text": "When you start programming, many operations seem like simple instructions executed in sequence: load data, apply a function, get a result. As projects grow, another need emerges: organizing code and data in a way that is easier to understand, reuse, and extend. Object-oriented programming (OOP) arises as a response to this organizational problem.\nThinking in terms of object orientation involves slightly shifting the focus. Instead of concentrating solely on actions, attention is placed on the entities being acted upon and the relationships between data and operations. This way of structuring code is not exclusive to one language but rather a method for modeling problems.\n\nThe Core Idea of Object Orientation\nGenerally speaking, object-oriented programming is based on the concept of an object. An object combines two things: data and the operations that make sense for that data. For instance, a dataset can have associated operations for summarization, plotting, or transformation.\nObjects usually belong to a class, which defines what kind of data they contain and what operations can be applied. Concrete objects are created from a class. This separation allows working with abstractions: there’s no need to know all the internal details to use an object consistently.\nAnother important concept is that of a method, which is a function associated with a specific class. Instead of thinking of a function as existing in isolation, it is understood as an operation applied to a particular type of object.\n\n\nWhy This Logic Is Useful\nObject orientation helps manage complexity. It allows writing code that adapts to different data types without being completely rewritten, facilitates the extension of functionalities, and promotes a certain internal coherence. For those reading the code, it also offers clues about which operations are expected for each type of object.\nThis way of organizing work is especially valuable in languages used for data analysis, where tables, statistical models, plots, and intermediate results coexist. Each of these elements can be thought of as an object with its own behaviors.\n\n\nR as an Object-Oriented Language\nR is an object-oriented language, although it’s not always perceived that way when you start using it. This is because R implements object orientation in a particular way, less explicit than in other languages like Java or Python.\nIn R, almost everything is an object. A vector, a data frame, a statistical model, or a plot are objects with an associated class. That class defines how they behave with certain functions. For example, the summary() function produces different results depending on the type of object it receives. The same function name activates different methods depending on the class of the input object.\nThis mechanism is known as method dispatch. Instead of calling a specific function for each data type, R internally decides which method to use. For data analysts, this reduces cognitive load: the same general functions are used, and the behavior adjusts to the object.\n\n\nObject Systems in R\nR has several object systems. The most well-known are S3 and S4, and more recently R6. S3 is a flexible and informal system, widely used in packages and in everyday use. S4 is stricter and explicitly defines the structure of classes. R6 is more similar to classic object orientation, with mutable objects.\nTo start, it’s not necessary to master these systems in detail. The important thing is to recognize that when working with data frames, models, or plots, you are interacting with objects that respond to specific rules according to their class.\n\n\nReading Code from This Perspective\nUnderstanding that R is an object-oriented language helps to better read code. It allows you to ask what type of object you are working with, what operations are consistent with that object, and why a function behaves in a certain way. This perspective makes analysis more predictable and facilitates learning new packages.\nObject orientation in R does not require writing classes from day one. It often functions as a silent infrastructure that organizes the language. Recognizing its presence helps to move from executing code to interpreting it as part of a broader system."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning to think with data",
    "section": "",
    "text": "A space to understand what you’re doing when you analyze, model, and visualize.\n\nSubscribe to the newsletter See latest articles"
  },
  {
    "objectID": "index.html#latest-articles",
    "href": "index.html#latest-articles",
    "title": "Learning to think with data",
    "section": "Latest articles",
    "text": "Latest articles\n\n\n\n\n\nWhat is object-oriented programming and why is R considered an object-oriented language?\n\n\nWhat does it mean for a language to be object-oriented? This post introduces the concept of object-oriented programming and explains how R implements this logic through classes, objects, and methods in daily data work.\n\n\n\n\n\nJan 29, 2026\n\n\n\n\n\n\n\nWhat is Data Science and Why All the Interest?\n\n\nWhat is data science and where does it come from? This post explores its origin, its link to statistics and big data, and analyzes why more and more people are approaching this field from diverse analytical traditions.\n\n\n\n\n\nJan 28, 2026\n\n\n\n\n\n\n\n6 Essential Books for Learning Data Science\n\n\nFive books available online for free for an introduction and in-depth study of data science, from fundamentals and communication to programming with R and applied statistics.\n\n\n\n\n\nJan 27, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#one-email-a-month",
    "href": "index.html#one-email-a-month",
    "title": "Learning to think with data",
    "section": "One email a month",
    "text": "One email a month\nNew articles. Reflections on analysis, models, and methodological decisions.\nNews that caught our attention and that we want to share.\nNo noise. No empty lists.\n\n\n\n    \n\n\n    \n      \n        \n\n          \n\n            \n              \n            \n\n            \n              \n                \n\n                  \n                  \n                    \n\n                      \n\n\n                      \n                      \n                      \n\n                      \n                      \n                      \n\n                      \n                      \n                      \n\n                      \n            \n            \n\n                      \n                      \n                      \n\n                      \n                      \n                      \n\n                    \n                  \n                \n              \n\n              \n\n              \n                \n                  Subscribe\n                \n                \n                  \n                  Loading...\n                \n              \n\n              \n            \n          \n\n          \n            \n              Thank you!\n              You have successfully joined our subscriber list."
  },
  {
    "objectID": "index.html#what-you-can-learn-here",
    "href": "index.html#what-you-can-learn-here",
    "title": "Learning to think with data",
    "section": "What you can learn here",
    "text": "What you can learn here\nR programming applied to real data\nClear guides for loading, transforming, and visualizing data with tidyverse, ggplot2, and reproducible workflows.\nQuantitative analysis with methodological rigor\nAccessible explanations of models, analytical decisions, and statistical tools, designed for social science and educational research.\nData science fundamentals\nIntroductory and in-depth texts on machine learning, reproducibility, and analysis project design."
  },
  {
    "objectID": "index.html#this-space-is-growing",
    "href": "index.html#this-space-is-growing",
    "title": "Learning to think with data",
    "section": "This space is growing",
    "text": "This space is growing\nIn the coming months you’ll find:\n\nDownloadable materials\nStructured courses\nSupport for data analysis projects\n\nBecause learning data is not just consuming content: it’s practicing, making mistakes, and thinking again."
  }
]